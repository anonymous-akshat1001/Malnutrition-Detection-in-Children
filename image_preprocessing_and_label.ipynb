{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c7b5a20f",
      "metadata": {},
      "source": [
        "# Image Labelling and Preprocessing\n",
        "\n",
        "This notebook handles the **image labeling** and **preprocessing** steps for the Malnutrition Classification Project.\n",
        "It assumes the dataset consists of images corresponding to different children in multiple poses (e.g., frontal, lateral, back, etc.).\n",
        "The goal is to map each image to its label (malnourished / not malnourished) and prepare it for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6da33e79",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary packages\n",
        "import os\n",
        "import re\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import random\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "except Exception:\n",
        "    cv2 = None\n",
        "\n",
        "try:\n",
        "    import mediapipe as mp\n",
        "except Exception:\n",
        "    mp = None\n",
        "\n",
        "try:\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "except Exception:\n",
        "    A = None\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b79797",
      "metadata": {},
      "source": [
        "# Utilities : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a01ab90e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compiles a regular expression (regex) pattern into an object. Regex is used to match strings that follow a certain pattern.\n",
        "# (?P<index>\\d+) → Named group index. \\d+ means one or more digits. For example, 0000 will be captured as index.\n",
        "FILENAME_RE = re.compile(r\"^(?P<index>\\d+)[_](?P<child>\\d+).*[_](?P<pose>[^.]+)\\.(?:jpg|jpeg|png)$\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "# These are standard mean and standard deviation values used when normalizing images for models trained on ImageNet.\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "# Extract index, child_id and pose from filename. Returns (index, child_id, pose) or (None, None, None)\n",
        "def parse_filename(fname: str):\n",
        "    # Tries to match the filename with the regex we defined\n",
        "    m = FILENAME_RE.match(fname)\n",
        "    if m:\n",
        "        return m.group('index'), m.group('child'), m.group('pose')\n",
        "    # fallback: split(stem->file name without extension) and try\n",
        "    parts = Path(fname).stem.split('_')\n",
        "    if len(parts) >= 3:\n",
        "        return parts[0], parts[1], parts[-1]\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "# images_dir: Path → The folder where all images are stored as argument to function\n",
        "def find_images(images_dir: Path):\n",
        "    # Stores image info\n",
        "    imgs = []\n",
        "    # Recursively search all files in images_dir and subfolders\n",
        "    for p in images_dir.rglob('*'):\n",
        "        if p.suffix.lower() in ('.jpg', '.jpeg', '.png') and p.is_file():\n",
        "            idx, child, pose = parse_filename(p.name)\n",
        "            # Skip or include with unknown child\n",
        "            if child is None:\n",
        "                continue\n",
        "            imgs.append({'path': str(p), 'index': idx, 'child_id': str(int(child)), 'pose': pose.lower()})\n",
        "    # Get a structured table of all images and their metadata\n",
        "    return pd.DataFrame(imgs)\n",
        "\n",
        "\n",
        "# Function returns a clean DataFrame mapping each child ID to its label, ready to merge with image metadata\n",
        "def read_labels_csv(labels_csv: Path):\n",
        "\n",
        "    df = pd.read_csv(labels_csv)\n",
        "\n",
        "    # Use the correct columns based on your CSV headers\n",
        "    child_col = 'tag'\n",
        "    label_col = 'binary_label'\n",
        "\n",
        "    # Select only relevant columns and rename for standardization\n",
        "    labels = df[[child_col, label_col]].copy()\n",
        "    labels.columns = ['child_id', 'label']\n",
        "\n",
        "    # Ensure child IDs are strings and remove any '.0' if present\n",
        "    labels['child_id'] = labels['child_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "    # Remove rows with missing labels\n",
        "    labels = labels[labels['label'].notnull()]\n",
        "\n",
        "    # Drop duplicates in case the same child appears multiple times\n",
        "    return labels.drop_duplicates(subset=['child_id'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ed9310",
      "metadata": {},
      "source": [
        "# MediaPipe / Keypoints & BBoxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a5ca3f13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keypoints (also called landmarks) are specific, meaningful points on a human body. Used for Body measurements\n",
        "# For example: head, shoulders, elbows, wrists, hips, knees, ankles, etc.\n",
        "# Each keypoint usually has x, y coordinates in the image, and sometimes a visibility score (how confident the model is that the point is visible).\n",
        "\n",
        "# A bounding box is a rectangle that tightly encloses the object of interest (in our case, the child).\n",
        "\n",
        "# The following code deals with extracting body keypoints and bounding boxes using MediaPipe, a library for pose estimation\n",
        "\n",
        "class PoseExtractor:\n",
        "\n",
        "    def __init__(self, use_mediapipe=True):\n",
        "        # instance variable that stores whether we should use MediaPipe(imported as mp) or not\n",
        "        self.use_mediapipe = use_mediapipe and (mp is not None)\n",
        "        if self.use_mediapipe:\n",
        "            # gives access to the pre-trained pose detection model\n",
        "            self.mp_pose = mp.solutions.pose\n",
        "            # tells that we are using static images and sensitivity of pose detection(Lower = more sensitive, Higher = more precise)\n",
        "            self.pose = self.mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.4)\n",
        "\n",
        "\n",
        "    # Argument to method : the image we want to process, represented as a NumPy array in BGR format (OpenCV standard)\n",
        "    # Returns dict with 'keypoints' (landmark_name -> (x,y,visibility)) and 'bbox' (x_min,y_min,x_max,y_max) or None if not found\n",
        "    def extract(self, image_bgr: np.ndarray):\n",
        "        \n",
        "        # store the width and height\n",
        "        h, w = image_bgr.shape[:2]\n",
        "        \n",
        "        # Dictionary which stores keypoints and bounding box\n",
        "        result = {'keypoints': {}, 'bbox': None}\n",
        "\n",
        "        if not self.use_mediapipe:\n",
        "            return result\n",
        "        \n",
        "        # convert to RGB for mediapipe(OpenCV uses BGR)\n",
        "        img_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Run the pose detection model on the image. res containns the landmarks/keypoints\n",
        "        res = self.pose.process(img_rgb)\n",
        "\n",
        "        if not res.pose_landmarks:\n",
        "            return result\n",
        "        \n",
        "        # lms is a list of landmarks (keypoints) detected. Each landmark has:\n",
        "        # x, y, z: normalized coordinates (0–1)\n",
        "        # visibility: likelihood the landmark is visible\n",
        "        lms = res.pose_landmarks.landmark\n",
        "\n",
        "        kp = {}     # Dict to store keypoints\n",
        "        xs = []     # x coordinates for bounding box\n",
        "        ys = []     # y coordinates for bounding box\n",
        "\n",
        "        # Loop through each landmark\n",
        "        for i, lm in enumerate(lms):\n",
        "            # Multiply with image width to convert normalized to pixels coordinates\n",
        "            x = lm.x * w\n",
        "            y = lm.y * h\n",
        "            v = lm.visibility\n",
        "            xs.append(x)\n",
        "            ys.append(y)\n",
        "            kp[i] = (x, y, v)\n",
        "\n",
        "        # Minimal set of keypoints of interest — map to indices if needed\n",
        "        result['keypoints'] = kp\n",
        "\n",
        "        # Compute bounding box\n",
        "        x_min = max(0, int(min(xs)))    # leftmost x-coordinate of any keypoint\n",
        "        x_max = min(w, int(max(xs)))\n",
        "        y_min = max(0, int(min(ys)))    # topmost y-coordinate\n",
        "        y_max = min(h, int(max(ys)))\n",
        "\n",
        "        # expand bbox by 10% padding\n",
        "        pad_x = int((x_max - x_min) * 0.1)\n",
        "        pad_y = int((y_max - y_min) * 0.1)\n",
        "        result['bbox'] = [max(0, x_min - pad_x), max(0, y_min - pad_y), min(w, x_max + pad_x), min(h, y_max + pad_y)]\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162caf8d",
      "metadata": {},
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "83f8cbc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return an albumentations augmentation pipeline. Will be applied consistently per-child when needed.\n",
        "\n",
        "def make_augmentations(output_size=(224, 224), training=True):\n",
        "\n",
        "    # A is the Albumentations library imported earlier\n",
        "    if A is None:\n",
        "        raise RuntimeError('Albumentations is required for augmentations. pip install albumentations')\n",
        "\n",
        "    # Normalization adjusts pixel values to a standard range, which helps neural networks train faster and more stably\n",
        "    norm = A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "\n",
        "    if training:\n",
        "        aug = A.Compose([\n",
        "\n",
        "            # Randomly crops a part of the image and resizes it to output_size\n",
        "            A.RandomResizedCrop(\n",
        "                size=output_size, \n",
        "                scale=(0.8, 1.0), \n",
        "                ratio=(0.9, 1.1), \n",
        "                p=1.0\n",
        "            ),\n",
        "            \n",
        "            # Randomly flips the image left-right 25% of the time\n",
        "            A.HorizontalFlip(p=0.25),\n",
        "\n",
        "            # choose one of the listed augmentations 60% of the time\n",
        "            A.OneOf([\n",
        "                # Shift + zoom + rotate\n",
        "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=20, p=0.6),\n",
        "                # Skews the image along x or y axis by shear percent\n",
        "                A.Affine(shear=10, p=0.4),\n",
        "            ], p=0.6),\n",
        "\n",
        "            # Randomly adjusts brightness, contrast, hue, saturation.\n",
        "            A.OneOf([\n",
        "                A.RandomBrightnessContrast(p=0.6),\n",
        "                A.HueSaturationValue(p=0.6),\n",
        "            ], p=0.5),\n",
        "\n",
        "            # occlusion / coarse dropout (Randomly blacks out a small rectangle in the image)\n",
        "            A.CoarseDropout(max_holes=1, max_height=int(0.15*output_size[0]), max_width=int(0.15*output_size[1]), min_holes=1, p=0.3),\n",
        "\n",
        "            norm,\n",
        "\n",
        "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)\n",
        "        # Last line tells that we may have landmarks/keypoints and keep then even if it goes outside image after transformation\n",
        "    )\n",
        "\n",
        "    else:\n",
        "        # For validation/test images, we do not apply random augmentations, we only resize and normalize\n",
        "        aug = A.Compose(\n",
        "            [\n",
        "                A.Resize(*output_size), \n",
        "                norm\n",
        "            ], \n",
        "            keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)\n",
        "        )\n",
        "    \n",
        "    return aug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3e7ac5",
      "metadata": {},
      "source": [
        "# Processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3adae624",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load image, optionally crop to bbox, apply augmentation pipeline and save output image and keypoints if provided by aug\n",
        "\n",
        "def process_and_save_image(img_path, out_path, bbox=None, aug=None):\n",
        "\n",
        "    # loads an image as a NumPy array in BGR format\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        raise RuntimeError(f'Could not read image: {img_path}')\n",
        "    \n",
        "    # Original dimensions\n",
        "    orig_h, orig_w = img.shape[:2]\n",
        "    \n",
        "    # Crop according to bounding box\n",
        "    if bbox is not None:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        # ensure ints\n",
        "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "        # clip - Ensures the bounding box doesn’t go outside the image\n",
        "        x1 = max(0, min(x1, orig_w-1))\n",
        "        x2 = max(0, min(x2, orig_w))\n",
        "        y1 = max(0, min(y1, orig_h-1))\n",
        "        y2 = max(0, min(y2, orig_h))\n",
        "        # Check for invalid boxes\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            cropped = img\n",
        "        else:\n",
        "            cropped = img[y1:y2, x1:x2]\n",
        "    else:\n",
        "        cropped = img\n",
        "\n",
        "    # convert BGR->RGB for albumentations\n",
        "    rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply data augmenta\n",
        "    if aug is not None:\n",
        "        res = aug(image=rgb)\n",
        "        rgb = res['image']\n",
        "\n",
        "    # convert back to BGR for saving via cv2\n",
        "    if rgb.dtype == np.float32 or rgb.dtype == np.float64:\n",
        "        rgb_uint8 = np.clip(rgb * 255.0, 0, 255).astype(np.uint8)   # ensures pixel values are in the valid range and integers\n",
        "    else:\n",
        "        rgb_uint8 = rgb\n",
        "    final_bgr = cv2.cvtColor(rgb_uint8, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Creates the folder where the output image will be saved\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    cv2.imwrite(out_path, final_bgr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5448f739",
      "metadata": {},
      "source": [
        "# MAIN DRIVER"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec17c8df",
      "metadata": {},
      "source": [
        "Define path and parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6b173b7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "root = Path('.')  # Project root\n",
        "images_dir = root / 'Anthrovision' / 'merged_dataset'\n",
        "labels_csv = root / 'Anthrovision' / 'anthrovision_labels.csv'\n",
        "out_dir = root / 'Anthrovision' \n",
        "\n",
        "# A manifest CSV will track all processed images, their labels, child IDs, split (train/val/test), and keypoints\n",
        "manifest_path = out_dir / 'manifest.csv'\n",
        "\n",
        "size = 224\n",
        "train_frac = 0.7\n",
        "val_frac = 0.15\n",
        "test_frac = 0.15\n",
        "seed = 42\n",
        "use_mediapipe = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "089725fa",
      "metadata": {},
      "source": [
        "Scan Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "63f841d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 16935 images belonging to 2139 unique children\n"
          ]
        }
      ],
      "source": [
        "# Get a structured table of all images and their metadata\n",
        "images_df = find_images(images_dir)\n",
        "\n",
        "# Reports total images and unique children\n",
        "print(f\"Found {len(images_df)} images belonging to {images_df.child_id.nunique()} unique children\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a172ce6",
      "metadata": {},
      "source": [
        "Read Labels and Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "24db9494",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function returns a clean DataFrame mapping each child ID to its label\n",
        "labels_df = read_labels_csv(labels_csv)\n",
        "\n",
        "# Merges image metadata with label data by child_id\n",
        "merged = images_df.merge(labels_df, on='child_id', how='left')\n",
        "\n",
        "# Drop images without labels\n",
        "merged = merged[merged['label'].notnull()]\n",
        "\n",
        "# Ensures labels are integers, which is required for classification\n",
        "label_mapping = {\n",
        "    'healthy': 0,\n",
        "    'malnourished': 1\n",
        "}\n",
        "\n",
        "# Apply mapping\n",
        "merged['label'] = merged['label'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15af45ef",
      "metadata": {},
      "source": [
        "Compute Per Child Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "61302a7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop images with NaN labels first\n",
        "merged = merged[merged['label'].notnull()]\n",
        "\n",
        "# Ensure label is numeric\n",
        "merged['label'] = merged['label'].astype(int)\n",
        "\n",
        "# Some children have multiple images; here we compute one label per child:\n",
        "# Use mode (most frequent label) across all images. If no mode, use the first label.\n",
        "def child_label_func(x):\n",
        "    x = x.dropna()  # remove NaNs\n",
        "    if len(x) == 0:\n",
        "        return None   # if no valid labels\n",
        "    mode_vals = x.mode()\n",
        "    if len(mode_vals) > 0:\n",
        "        return int(mode_vals.iloc[0])\n",
        "    else:\n",
        "        return int(x.iloc[0])\n",
        "\n",
        "child_label = merged.groupby('child_id')['label'].agg(child_label_func).reset_index()\n",
        "\n",
        "# Remove children with no label at all (if any)\n",
        "child_label = child_label[child_label['label'].notnull()]\n",
        "\n",
        "children = child_label['child_id'].tolist()\n",
        "labels = child_label['label'].tolist()\n",
        "\n",
        "# Split\n",
        "train_ids, temp_ids, train_lbls, temp_lbls = train_test_split(\n",
        "    children, labels, test_size=(1-train_frac), random_state=seed, stratify=labels\n",
        ")\n",
        "\n",
        "rel = val_frac / (val_frac + test_frac)\n",
        "val_ids, test_ids, _, _ = train_test_split(\n",
        "    temp_ids, temp_lbls, test_size=(1-rel), random_state=seed, stratify=temp_lbls\n",
        ")\n",
        "\n",
        "# Assign split per image\n",
        "def assign_split(row):\n",
        "    cid = row['child_id']\n",
        "    if cid in train_ids: return 'train'\n",
        "    if cid in val_ids: return 'val'\n",
        "    if cid in test_ids: return 'test'\n",
        "    return 'none'\n",
        "\n",
        "merged['split'] = merged.apply(assign_split, axis=1)\n",
        "merged = merged[merged['split'] != 'none']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55066886",
      "metadata": {},
      "source": [
        "Initialize extractor and augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fa74b2b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16556\\4141687529.py:41: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(max_holes=1, max_height=int(0.15*output_size[0]), max_width=int(0.15*output_size[1]), min_holes=1, p=0.3),\n"
          ]
        }
      ],
      "source": [
        "# PoseExtractor will detect keypoints/landmarks and bounding boxes for each child image\n",
        "extractor = PoseExtractor(use_mediapipe=use_mediapipe)\n",
        "\n",
        "# Sets up image augmentations\n",
        "train_aug = make_augmentations(output_size=(size, size), training=True)\n",
        "val_aug = make_augmentations(output_size=(size, size), training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ecf0838",
      "metadata": {},
      "source": [
        "Process images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b54487d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train:   0%|          | 0/1497 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train: 100%|██████████| 1497/1497 [24:42<00:00,  1.01it/s]\n",
            "Processing val: 100%|██████████| 321/321 [05:10<00:00,  1.03it/s]\n",
            "Processing test: 100%|██████████| 321/321 [05:19<00:00,  1.01it/s]\n"
          ]
        }
      ],
      "source": [
        "# A dictionary for info about processed images\n",
        "records = []\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    \n",
        "    # sub is the subset of images that belong to the current split.\n",
        "    sub = merged[merged['split'] == split]\n",
        "\n",
        "    # Loop over children within the split. tqdm is a progress bar for processing\n",
        "    for child_id, group in tqdm(sub.groupby('child_id'), desc=f'Processing {split}'):\n",
        "\n",
        "        # Ensures deterministic augmentations. Same child → same random transforms. Different children → different transforms.\n",
        "        seed_child = int(child_id) if child_id.isdigit() else abs(hash(child_id)) % (2**31)\n",
        "        random.seed(seed_child)\n",
        "        np.random.seed(seed_child)\n",
        "        \n",
        "        # Choose the augmentation pipeline\n",
        "        aug = train_aug if split=='train' else val_aug\n",
        "\n",
        "        # Loop over each image of the child\n",
        "        for _, row in group.iterrows():\n",
        "            # loads the image in BGR format\n",
        "            img_path = row['path']\n",
        "            img_bgr = cv2.imread(img_path)\n",
        "            \n",
        "            # Keypoints and bounding box\n",
        "            kp_data = None\n",
        "            bbox = None\n",
        "            \n",
        "            # Extract keypoints and bounding box\n",
        "            if img_bgr is not None and extractor.use_mediapipe:\n",
        "                res = extractor.extract(img_bgr)\n",
        "                kp_data = res.get('keypoints', {})\n",
        "                bbox = res.get('bbox', None)\n",
        "\n",
        "            # Build output path\n",
        "            rel_out = Path(row['split']) / f\"child_{row['child_id']}\" / Path(img_path).name\n",
        "            out_path = out_dir / rel_out\n",
        "\n",
        "            process_and_save_image(img_path, str(out_path), bbox=bbox, aug=aug)\n",
        "\n",
        "            # Create record for manifest\n",
        "            record = {\n",
        "                'orig_path': img_path,\n",
        "                'proc_path': str(out_path),\n",
        "                'child_id': row['child_id'],\n",
        "                'pose': row['pose'],\n",
        "                'label': int(row['label']),\n",
        "                'split': row['split']\n",
        "            }\n",
        "\n",
        "            # Save the keypoints data\n",
        "            if kp_data:\n",
        "                kp_file = str(out_path) + '.kps.json'\n",
        "                with open(kp_file, 'w') as fh:\n",
        "                    json.dump(kp_data, fh)\n",
        "                record['keypoints_file'] = kp_file\n",
        "\n",
        "            records.append(record)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d64eeee",
      "metadata": {},
      "source": [
        "Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b2528b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed dataset saved to Anthrovision\n",
            "Manifest saved to Anthrovision\\manifest.csv\n"
          ]
        }
      ],
      "source": [
        "manifest = pd.DataFrame(records)\n",
        "manifest.to_csv(manifest_path, index=False)\n",
        "print('Processed dataset saved to', out_dir)\n",
        "print('Manifest saved to', manifest_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
