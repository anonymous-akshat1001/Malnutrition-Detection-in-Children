{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbe5f1c",
   "metadata": {},
   "source": [
    "# CODE FOR THE FULL MULTITASKING PIPELINE - EXTRACTS ANTHOPOMETRIC MEASURES AND CLASSIFICATION OF IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1748af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %pip uninstall torch torchvision torchaudio -y\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, root_mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4a114",
   "metadata": {},
   "source": [
    "# Decide the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ab4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Configurations : \n",
    "MODEL_CONFIGS = {\n",
    "    \"resnet50\": {\n",
    "        \"img_size\": 224,\n",
    "        \"weights\": \"IMAGENET1K_V1\",  \n",
    "        \"target_layer\" : \"layer4\"\n",
    "    },\n",
    "    \"efficientnet_b3\": {\n",
    "        \"img_size\": 300,\n",
    "        \"weights\": \"IMAGENET1K_V1\",\n",
    "        \"target_layer\" : \"features\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Choose which backbone you want to train\n",
    "BACKBONE = \"resnet50\"   # change to \"efficientnet_b3\" to train EffNet\n",
    "IMG_SIZE = MODEL_CONFIGS[BACKBONE][\"img_size\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef6ac5",
   "metadata": {},
   "source": [
    "# Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "SEED = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Paths \n",
    "MANIFEST = \"Anthrovision/manifest.csv\"  # your manifest with proc_path, child_id, label, split\n",
    "IMG_ROOT = \".\"              # base dir for proc_path (if proc_path already absolute, that's fine)\n",
    "\n",
    "# Model & training params\n",
    "BATCH_SIZE = 8                     # number of children per batch (each child has num_imgs)\n",
    "NUM_IMAGES_PER_CHILD = 5           # sample/pad to fixed number\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 30\n",
    "REGRESSION_LOSS_WEIGHT = 0.01       # weight for regression loss\n",
    "CLASSIFICATION_LOSS_WEIGHT = 0.5   # weight for classification loss\n",
    "GRAD_CLIP = 1.0\n",
    "PATIENCE_LR_SCHED = 4\n",
    "\n",
    "\n",
    "# For augmentation\n",
    "def make_transforms(img_size):\n",
    "    train_tf = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomApply(\n",
    "            [T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.02)],\n",
    "            p=0.5\n",
    "        ),\n",
    "        T.RandomRotation(degrees=10),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    val_tf = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "train_transforms, val_transforms = make_transforms(IMG_SIZE)\n",
    "\n",
    "print(f\"Using backbone: {BACKBONE} with image size {IMG_SIZE}x{IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef885bf",
   "metadata": {},
   "source": [
    "# Merging Manifest and Original Labels File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your original labels csv contains the numeric targets (e.g., head_circumference,height,muac),\n",
    "# merge them into manifest.csv keyed by child_id. If your manifest already contains them, skip this.\n",
    "\n",
    "def ensure_targets_in_manifest(manifest_path: str, original_labels_path: str = None):\n",
    "    df = pd.read_csv(manifest_path)\n",
    "\n",
    "    # Check for regression columns\n",
    "    reg_cols = ['HC', 'Height', 'MUAC']\n",
    "    \n",
    "    # Build list of which of those names are missing from df.columns\n",
    "    missing = [c for c in reg_cols if c not in df.columns]\n",
    "\n",
    "    # Try to merge manifest and the original_labels_path(anthrovision_lables.csv)\n",
    "    if missing and original_labels_path:\n",
    "        print(f\"Manifest missing {missing}. Attempting to merge from {original_labels_path}\")\n",
    "        orig = pd.read_csv(original_labels_path)\n",
    "\n",
    "        print(orig.columns.tolist())\n",
    "\n",
    "        # orig should contain child_id(tag) + regression columns\n",
    "        df = df.merge(orig[['tag'] + reg_cols], left_on='child_id', right_on='tag', how='left')\n",
    "        \n",
    "        df.to_csv(manifest_path, index=False)\n",
    "        print(\"Merged and saved manifest with regression targets.\")\n",
    "    elif missing:\n",
    "        raise ValueError(f\"Manifest is missing regression target columns {missing}. Provide original_labels_path to merge.\")\n",
    "    else:\n",
    "        print(\"Manifest already contains regression target columns.\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2df6b",
   "metadata": {},
   "source": [
    "# Building Dataset - one item = one child with N images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ChildImageDataset is a custom PyTorch Dataset whose one item corresponds to one child (not one image).\n",
    "    Each item returns:\n",
    "        images: a tensor containing num_images_per_child images for that child, shape (num_images, C, H, W).\n",
    "        reg_targets: tensor of 3 regression values [head_circumference, height, muac] (may contain nan when missing).\n",
    "        class_label: tensor 0.0 or 1.0 for malnutrition binary target.\n",
    "        child_id: identifier string for that child\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_csv: str, split: str, img_root: str = \".\", transforms=None, num_images_per_child: int = 4):\n",
    "        self.df = pd.read_csv(manifest_csv)\n",
    "        self.split = split                                        # split -> test , val , train\n",
    "        self.img_root = img_root\n",
    "        self.transforms = transforms\n",
    "        self.num_images_per_child = num_images_per_child\n",
    "\n",
    "        # Filter split to keep only rows where the split column (case-insensitive) matches the requested split\n",
    "        self.split_df = self.df[self.df['split'].str.lower() == split.lower()].copy()\n",
    "        if self.split_df.empty:\n",
    "            raise ValueError(f\"No rows found for split={split} in {manifest_csv}\")\n",
    "\n",
    "        # Groups rows that belong to the same child. Each group g is a small DataFrame containing all rows/images for one child\n",
    "        self.children = []\n",
    "        child_groups = self.split_df.groupby('child_id')\n",
    "\n",
    "        for child_id, g in child_groups:\n",
    "            # List of all image file paths for this child\n",
    "            paths = g['proc_path'].tolist()      \n",
    "            # List of the binary labels for the images\n",
    "            labels = g['label'].astype(float).tolist()\n",
    "            \n",
    "            # For child-level targets (head_circumference, height, muac), each image row may include the same value\n",
    "            # Guarantees one child-level numeric value per child\n",
    "            def first_nonnull(col):\n",
    "                if col in g.columns : \n",
    "                    vals = g[col].dropna().unique().tolist()  \n",
    "                else: \n",
    "                    vals = []\n",
    "                return float(vals[0]) if vals else np.nan\n",
    "\n",
    "            head_circ = first_nonnull('HC')\n",
    "            height = first_nonnull('Height')\n",
    "            muac = first_nonnull('MUAC')\n",
    "\n",
    "            # Handles the rare case when labels differ per-image; majority vote produces a single child-level label\n",
    "            class_label = float(round(np.mean(labels)))\n",
    "\n",
    "            self.children.append({\n",
    "                'child_id': child_id,\n",
    "                'paths': paths,          # List of image paths\n",
    "                'class_label': class_label,\n",
    "                'head_circumference': head_circ,\n",
    "                'height': height,\n",
    "                'muac': muac\n",
    "            })\n",
    "\n",
    "    # Returns number of unique children in the chosen split\n",
    "    def __len__(self):\n",
    "        return len(self.children)\n",
    "\n",
    "    def _load_and_transform(self, path):\n",
    "        # Allow proc_path be absolute or relative\n",
    "        if not os.path.isabs(path):\n",
    "            p = os.path.join(self.img_root, path)\n",
    "        else:\n",
    "            p = path\n",
    "\n",
    "        # Ensures consistent 3-channel input(RGB) even if some images are grayscale    \n",
    "        img = Image.open(p).convert('RGB')\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        else:\n",
    "            img = T.ToTensor()(img)\n",
    "\n",
    "        # Return tensor : (Channel = 3 , Height , Width)\n",
    "        return img\n",
    "\n",
    "\n",
    "    # Fetch the idx-th item\n",
    "    def __getitem__(self, idx):\n",
    "        # dictionary we built for the child at that index\n",
    "        info = self.children[idx]\n",
    "        imgs = []\n",
    "\n",
    "        # Randomly sample num_images_per_child unique images from the child's available images - in case of more images\n",
    "        if len(info['paths']) >= self.num_images_per_child:\n",
    "            # sample randomly, but keep deterministic seed per idx/epoch could be added\n",
    "            sampled = random.sample(info['paths'], self.num_images_per_child)\n",
    "        # In case of less images\n",
    "        else:\n",
    "            # repeat first image to pad\n",
    "            sampled = list(info['paths'])\n",
    "            while len(sampled) < self.num_images_per_child:\n",
    "                sampled.append(info['paths'][0])  # repeat first\n",
    "                \n",
    "        # For each selected path p, load the image and transform it\n",
    "        for p in sampled:\n",
    "            imgs.append(self._load_and_transform(p))\n",
    "        imgs_tensor = torch.stack(imgs, dim=0)  # (num_images, C, H, W)\n",
    "\n",
    "        # building 1D tensor of regression targets: could be NaN if missing; we will mask them during loss\n",
    "        reg_targets = torch.tensor([\n",
    "            float(info['head_circumference']) if not pd.isna(info['head_circumference']) else np.nan,\n",
    "            float(info['height']) if not pd.isna(info['height']) else np.nan,\n",
    "            float(info['muac']) if not pd.isna(info['muac']) else np.nan,\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'images': imgs_tensor,\n",
    "            'reg_targets': reg_targets,\n",
    "            'class_label': torch.tensor(info['class_label'], dtype=torch.float32),\n",
    "            'child_id': info['child_id']\n",
    "        }\n",
    "\n",
    "# collate_fn is default since each item has same shapes (we fixed num_images_per_child)\n",
    "# collate_fn will stack items into batch tensors\n",
    "def get_dataloaders(manifest_path: str, img_root: str, batch_size: int, num_images_per_child: int, train_transforms, val_transforms):\n",
    "    \n",
    "    # Create dataset instances\n",
    "    train_ds = ChildImageDataset(\n",
    "        manifest_path, split='train', img_root=img_root,\n",
    "        transforms=train_transforms, num_images_per_child=num_images_per_child\n",
    "    )\n",
    "    val_ds = ChildImageDataset(\n",
    "        manifest_path, split='val', img_root=img_root,\n",
    "        transforms=val_transforms, num_images_per_child=num_images_per_child\n",
    "    )\n",
    "    test_ds = ChildImageDataset(\n",
    "        manifest_path, split='test', img_root=img_root,\n",
    "        transforms=val_transforms, num_images_per_child=num_images_per_child\n",
    "    )\n",
    "\n",
    "    # Create Dataloaders\n",
    "    # batch_size: number of children per batch (B). The effective number of images processed per GPU step will be B * num_images_per_child\n",
    "    # shuffle randomizes order of children each epoch\n",
    "    # num_workers spawns 4 worker processes to load and preprocess images in parallel\n",
    "    # pin_memory places tensors in page-locked memory to speed transfer to GPU\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c9ca5",
   "metadata": {},
   "source": [
    "# Building a MultiTask Model : ResNet50 + Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiTaskModel is a PyTorch neural network that extracts features from images using a ResNet50 backbone\n",
    "# It has two heads - classification + regression\n",
    "\n",
    "class ResNet_MultiTaskModel(nn.Module):\n",
    "    # Three regression outputs - muac , hc , height\n",
    "    def __init__(self, backbone_name='resnet50', pretrained=True, regression_out=3):\n",
    "        super().__init__()                                    # Initialize the pre-trained NN model\n",
    "        # Load backbone\n",
    "        if backbone_name == 'resnet50':\n",
    "            base = models.resnet50(pretrained=pretrained)     # pretrained weights trained on ImageNet are loaded\n",
    "            feat_dim = base.fc.in_features                    # usually 2048 - fully connected layer\n",
    "            # nn.Identity() simply returns its input unchanged. We do this because we want to attach our own heads\n",
    "            base.fc = nn.Identity()                           # we will use the pooled features\n",
    "            self.backbone = base\n",
    "            self.last_conv_name = 'layer4'                    # for grad-cam if needed - records last convolution layer\n",
    "        else:\n",
    "            raise NotImplementedError(\"Backbone not implemented in this script. Only ResNet50 allowed.\")\n",
    "\n",
    "        # Store feature dimensionality (e.g., 2048) for building heads.\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        # Building the Regression Heads- feed forward network\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 512),                  # Fully connected layer that maps the feature vector to 512 units \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),                           # Dropout prob = 30%\n",
    "            nn.Linear(512, regression_out)             # Final linear layer which has 3 output heads\n",
    "        )\n",
    "\n",
    "        # The final output is a single logit per example in the classification head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)                          # BCEWithLogitsLoss expects logits hence no sigmoid\n",
    "        )\n",
    "        # Note :  We do not put a sigmoid here. Instead we return logits; \n",
    "        # training should use BCEWithLogitsLoss, which combines a numerically stable sigmoid + binary cross-entropy internally\n",
    "\n",
    "    # Defines how the model computes outputs from inputs\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        images: Tensor (batch, num_imgs, C, H, W)\n",
    "        returns:\n",
    "          reg_preds (batch, 3), class_logits (batch, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack the tensor into variables\n",
    "        b, n, c, h, w = images.shape\n",
    "\n",
    "        # flatten images into batch*n - reshapes (without copying) the 5D tensor into a 4D tensor of shape\n",
    "        # This step is needed as the backbone expects a batch of single images\n",
    "        images_flat = images.view(b * n, c, h, w)\n",
    "\n",
    "        # Pass every image through the ResNet backbone\n",
    "        feats_flat = self.backbone(images_flat)  # feats_flat shape -> (b*n, feat_dim)\n",
    "\n",
    "        # Reshape back to group embeddings by child\n",
    "        feats = feats_flat.view(b, n, -1)        # feats shape -> (b, n, feat_dim)\n",
    "        # Now feats[i, j] is the embedding for the j-th image of the i-th child in the batch\n",
    "\n",
    "        # Average embeddings across images per child\n",
    "        pooled = feats.mean(dim=1)               # pooled shape -> (b, feat_dim)\n",
    "\n",
    "        # Pass the averaged embedding through the regression head\n",
    "        reg_out = self.regression_head(pooled)   # reg_out shape -> (b, 3)\n",
    "\n",
    "        # Pass through classification head\n",
    "        class_logits = self.classification_head(pooled).squeeze(1)  # shape(due to .squeeze) -> (b,)\n",
    "\n",
    "        return reg_out, class_logits, pooled "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221ef92",
   "metadata": {},
   "source": [
    "# Building a Multitask Model - EfficientNet-B3 + Attention Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module extracts per-image features with EfficientNet-B3, learns per-image attention weights \n",
    "# to fuse multiple views into a single child embedding, and from that embedding predicts \n",
    "# the three regression targets (head circumference, height, MUAC) and one classification logit (malnutrition). \n",
    "# It also returns the attention weights so you can inspect which images the model relied on.\n",
    "\n",
    "# Attention Pooling Module\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns to weigh each image embedding per child.\n",
    "    Input: feats -> (batch, num_imgs, feat_dim) , feat_dim is 1536 for EfficientNet-B3\n",
    "    Output: pooled -> (batch, feat_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "        # Creating a small MLP applied independently to each embedding vector\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),   # maps D(feature dimension) → 256, projects features into a small “attention space”.\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)           # maps the 256-vector to a single scalar score - unnormalized attention logit\n",
    "        )\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats: (B, N, D) , D is feature dimension\n",
    "        # Treats feats as a batch of size B*N vectors of length D and outputs B*N scalars, then reshapes them to (B, N, 1)\n",
    "        weights = self.attn(feats)                   # attention logit of image 'i' of child 'b'     \n",
    "        weights = torch.softmax(weights, dim=1)      # normalize weights across N images , logits -> weights\n",
    "        pooled = torch.sum(weights * feats, dim=1)   # weighted sum of features\n",
    "        return pooled, weights                       \n",
    "# Note : Averaging treats each view equally. Attention lets the network learn which views are useful \n",
    "# (e.g., frontal posture may be more informative for MUAC; lateral might help height). \n",
    "# This typically improves performance when image quality or pose varies.\n",
    "\n",
    "\n",
    "class EfficientNet_MultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone_name='efficientnet_b3', pretrained=True, regression_out=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load EfficientNet backbone\n",
    "        if backbone_name == 'efficientnet_b3':\n",
    "            base = models.efficientnet_b3(weights='IMAGENET1K_V1' if pretrained else None)    # loads EfficientNet-B3\n",
    "            feat_dim = base.classifier[1].in_features   # usually 1536\n",
    "            base.classifier = nn.Identity()             # remove classification head - we define our own output heads\n",
    "            self.backbone = base\n",
    "            self.last_conv_name = 'features'            # for Grad-CAM if needed\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backbone {backbone_name} not implemented\")\n",
    "\n",
    "        # Store feat_dim (1536) and create attention pooling instance\n",
    "        self.feat_dim = feat_dim\n",
    "        self.attn_pool = AttentionPooling(feat_dim)\n",
    "\n",
    "        # Regression head - no activation on final layer (raw continuous outputs)\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, regression_out)\n",
    "        )\n",
    "\n",
    "        # Classification head - last layer returns logits since BCEWithLogitsLoss expects logits\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)   \n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        images: Tensor (batch, num_imgs, C, H, W)\n",
    "        Returns:\n",
    "            reg_out: (batch, 3)\n",
    "            class_logits: (batch,)\n",
    "            pooled_feats: (batch, feat_dim)\n",
    "            attn_weights: (batch, num_imgs, 1)\n",
    "        \"\"\"\n",
    "        b, n, c, h, w = images.shape\n",
    "\n",
    "        # Reshape so backbone processes each image separately like a standard batch of single images\n",
    "        images_flat = images.view(b * n, c, h, w)\n",
    "\n",
    "        feats_flat = self.backbone(images_flat)        # image run through EfficientNet backbone, feat_flat -> (b*n, feat_dim)\n",
    "        feats = feats_flat.view(b, n, -1)              # Reshape back to group features per child -> (b, n, feat_dim)\n",
    "\n",
    "        # Attention pooling across images\n",
    "        pooled, attn_weights = self.attn_pool(feats)   # pooled -> (b, feat_dim)  , attn_weight -> (b, n, 1)\n",
    "\n",
    "        # Multi-task heads\n",
    "        reg_out = self.regression_head(pooled)                      # reg_out -> (b, 3)\n",
    "        class_logits = self.classification_head(pooled).squeeze(1)  # class_logits -> (b,) -> removes singleton channel dim\n",
    "\n",
    "        return reg_out, class_logits, pooled, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00050495",
   "metadata": {},
   "source": [
    "# Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean squared error (MSE) only for regression targets that actually exist. \n",
    "# Some children may not have HC/Height/MUAC (NaN). We ignore(mask) those entries in loss computation.\n",
    "def masked_regression_loss(preds, targets, mask=None, loss_fn=nn.SmoothL1Loss(reduction='none')):\n",
    "    \"\"\"\n",
    "    preds: (B, 3)  , our y_cap var which have been predicted by model\n",
    "    targets: (B, 3) with possible np.nan  ,  our y original variable\n",
    "    mask: optional boolean (B,3) where True indicates valid\n",
    "    \"\"\"\n",
    "    # If the caller doesn’t provide a mask, create one from targets: valid entries are where targets is not NaN\n",
    "    if mask is None:\n",
    "        mask = ~torch.isnan(targets)            # False indicates missing target(NaN)\n",
    "\n",
    "    # Contains squared error for each element with shape (B,3)\n",
    "    # loss_per_elem = (pred - targ)^2\n",
    "    loss_per_elem = loss_fn(preds, torch.nan_to_num(targets, nan=0.0))  \n",
    "\n",
    "    # Zero out invalid entries by multiplying with mask\n",
    "    loss_per_elem = loss_per_elem * mask.float()\n",
    "\n",
    "    # Average only over valid elements\n",
    "    valid_counts = mask.sum()         # Total number of valid target entries across the batch\n",
    "    \n",
    "    # In case of zero valid entries, avoids dividing by zero          \n",
    "    if valid_counts.item() == 0:\n",
    "        return torch.tensor(0.0, device=preds.device, requires_grad=True)\n",
    "    \n",
    "    # Otherwise average the sum of squared errors over the valid count → this is the masked MSE\n",
    "    return loss_per_elem.sum() / valid_counts.float()\n",
    "    # The returned scalar is the mean element-wise MSE across the whole batch for all available regression entries\n",
    "\n",
    "\n",
    "# Runs the model forward, computes the masked regression loss and a binary classification loss, \n",
    "# sums them (with configurable weights), backpropagates, and updates the optimizer\n",
    "def train_epoch(model, loader, optimizer, epoch, scheduler=None):\n",
    "    model.train()           # Sets the model in training mode       \n",
    "    total_loss = 0.0        # Accumulate the (weighted) loss across the epoch\n",
    "    total_samples = 0       # Counts the number of children processed\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1}\", leave=False)    # Progress bar\n",
    "\n",
    "    \n",
    "    scaler = torch.amp.GradScaler()\n",
    "    cls_loss_fn = nn.BCEWithLogitsLoss()            # sigmoid + binary cross entropy\n",
    "\n",
    "    for batch in loader:\n",
    "        images = batch['images'].to(device)                # (B, n, C, H, W)\n",
    "        reg_targets = batch['reg_targets'].to(device)     # (B, 3) may have NaN\n",
    "        class_labels = batch['class_label'].to(device)    # (B,)\n",
    "\n",
    "        # Clears old gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda'):    # allows GPU use\n",
    "            # The models accepts images and returns regression and classfication heads along with \n",
    "            # optionally pooled embeddings or attention weights that are ignored here(_)\n",
    "            outputs = model(images)\n",
    "            if len(outputs) >= 3:\n",
    "                if isinstance(outputs, (list, tuple)):\n",
    "                    reg_preds = outputs[0]\n",
    "                    class_logits = outputs[1]\n",
    "                else:\n",
    "                    reg_preds, class_logits = outputs\n",
    "            else:\n",
    "                reg_preds, class_logits = outputs\n",
    "                \n",
    "            # Check for NaNs/infs before computing losses\n",
    "            if torch.isnan(reg_preds).any() or torch.isinf(reg_preds).any():\n",
    "                print(\"NaN or inf in regression predictions!\")\n",
    "                print(\"Batch mean:\", reg_preds.mean().item(), \"std:\", reg_preds.std().item())\n",
    "                continue  # skip this batch\n",
    "\n",
    "            if torch.isnan(class_logits).any() or torch.isinf(class_logits).any():\n",
    "                print(\"NaN or inf in classification logits!\")\n",
    "                continue\n",
    "\n",
    "            # Regression loss (masked)\n",
    "            reg_loss = masked_regression_loss(reg_preds, reg_targets, None, nn.SmoothL1Loss(reduction='none'))\n",
    "            # Classification loss\n",
    "            cls_loss = cls_loss_fn(class_logits, class_labels)\n",
    "            # Weighted sum of regression and classification losses\n",
    "            loss = REGRESSION_LOSS_WEIGHT * reg_loss + CLASSIFICATION_LOSS_WEIGHT * cls_loss\n",
    "\n",
    "        # Backward propagation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping prevents explosion of gradients. clip_grad_norm_ rescales gradients if their global norm exceeds GRAD_CLIP\n",
    "        if GRAD_CLIP:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        # Updates parameters\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update() \n",
    "\n",
    "        # Accumulate total loss proportional to number of children\n",
    "        total_loss += loss.item() * images.shape[0]\n",
    "        # Total number of children trained\n",
    "        total_samples += images.shape[0]\n",
    "\n",
    "        # Updates the progress bar\n",
    "        pbar.set_postfix({\"cls\": f\"{cls_loss.item():.4f}\", \n",
    "                          \"reg\": f\"{reg_loss.item():.4f}\", \n",
    "                          \"tot\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Average loss per child is returned\n",
    "    avg_loss = total_loss / total_samples\n",
    "\n",
    "    # if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "    #     scheduler.step(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "# Runs the model over a loader without gradients and computes per-target regression metrics (MAE, RMSE, R²) and \n",
    "# classification metrics (accuracy, precision, recall, f1, ROC AUC)\n",
    "@torch.no_grad()            # disables gradient tracking to save memory and compute\n",
    "def evaluate(model, loader):\n",
    "    model.eval()            # Sets dropout off and BatchNorm into evaluation mode\n",
    "    # Collect true/pred for metrics, per-batch numpy arrays\n",
    "    all_reg_true = []\n",
    "    all_reg_pred = []\n",
    "    all_cls_true = []\n",
    "    all_cls_prob = []\n",
    "    all_child_ids = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Evaluating\", leave=False)\n",
    "\n",
    "    for batch in loader:\n",
    "        images = batch['images'].to(device, non_blocking=True)\n",
    "        reg_targets = batch['reg_targets'].to(device, non_blocking=True)\n",
    "        class_labels = batch['class_label'].to(device, non_blocking=True)\n",
    "        child_ids = batch['child_id']\n",
    "\n",
    "        outputs = model(images)\n",
    "        if len(outputs) >= 3:\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                reg_preds = outputs[0]\n",
    "                class_logits = outputs[1]\n",
    "            else:\n",
    "                reg_preds, class_logits = outputs\n",
    "        else:\n",
    "            reg_preds, class_logits = outputs\n",
    "        # Probabilities in [0,1] computed from logits by sigmoid\n",
    "        class_probs = torch.sigmoid(class_logits)\n",
    "\n",
    "        # Move to CPU numpy\n",
    "        all_reg_true.append(reg_targets.cpu().numpy())\n",
    "        all_reg_pred.append(reg_preds.cpu().numpy())\n",
    "        all_cls_true.append(class_labels.cpu().numpy())\n",
    "        all_cls_prob.append(class_probs.cpu().numpy())\n",
    "        all_child_ids.extend(child_ids)\n",
    "\n",
    "    # N is total children in the loader (sum of batch sizes), we now have the entire dataset predictions\n",
    "    all_reg_true = np.vstack(all_reg_true)       # (N,3)\n",
    "    all_reg_pred = np.vstack(all_reg_pred)\n",
    "    all_cls_true = np.concatenate(all_cls_true)  # (N,)\n",
    "    all_cls_prob = np.concatenate(all_cls_prob)\n",
    "\n",
    "    # Regression metrics: compute per target where targets not NaN\n",
    "    reg_metrics = {}\n",
    "    for i, name in enumerate(['head_circumference', 'height', 'muac']):\n",
    "        # Mark the valid targets\n",
    "        mask = ~np.isnan(all_reg_true[:, i])\n",
    "        # If no valid targets\n",
    "        if mask.sum() == 0:\n",
    "            reg_metrics[name] = {'MAE': None, 'RMSE': None, 'R2': None}\n",
    "            continue\n",
    "        # Compute accuracy metrics\n",
    "        y_true = all_reg_true[mask, i]\n",
    "        y_pred = all_reg_pred[mask, i]\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = root_mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        reg_metrics[name] = {'MAE': float(mae), 'RMSE': float(rmse), 'R2': float(r2)}\n",
    "\n",
    "    # Classification metrics \n",
    "    y_true_cls = all_cls_true\n",
    "    y_prob_cls = all_cls_prob\n",
    "\n",
    "    # Automatically find best threshold for F1 score - overcomes class imbalance\n",
    "    best_thresh, best_f1 = 0.5, 0.0\n",
    "    for t in np.linspace(0.05, 0.95, 91):   # test thresholds from 0.05 to 0.95\n",
    "        preds = (y_prob_cls >= t).astype(int)\n",
    "        f1 = f1_score(y_true_cls, preds, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thresh = f1, t\n",
    "\n",
    "    # Use best threshold for final predictions\n",
    "    y_pred_cls = (y_prob_cls >= best_thresh).astype(int)    # If prob > threshold , predict child is malnutritioned\n",
    "    \n",
    "    try:\n",
    "        roc_auc = float(roc_auc_score(y_true_cls, y_prob_cls))\n",
    "    except ValueError:\n",
    "        roc_auc = None  # if only one class present, since both are required\n",
    "    cls_metrics = {\n",
    "        'accuracy': float(accuracy_score(y_true_cls, y_pred_cls)),\n",
    "        'precision': float(precision_score(y_true_cls, y_pred_cls, zero_division=0)),\n",
    "        'recall': float(recall_score(y_true_cls, y_pred_cls, zero_division=0)),\n",
    "        'f1': float(f1_score(y_true_cls, y_pred_cls, zero_division=0)),\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "    return reg_metrics, cls_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f792243",
   "metadata": {},
   "source": [
    "# GRAD-CAM Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradCAM produces a visual explanation (a heatmap) that highlights \n",
    "# which parts of an input image the network used to make a particular decision\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Simple Grad-CAM for ResNet-like backbones. Usage:\n",
    "      cam = GradCAM(model, target_layer=model.layer4)\n",
    "      mask = cam.generate_cam(input_tensor, class_idx=None)\n",
    "    returns: mask (H_orig, W_orig) float between 0..1\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        # Use model.last_conv_name to find correct layer\n",
    "        if not hasattr(self.model, \"last_conv_name\"):\n",
    "            raise AttributeError(\"Model must define `last_conv_name` (e.g., 'layer4' or 'features').\")\n",
    "        \n",
    "        target_layer_name = self.model.last_conv_name\n",
    "        # Builds a dictionary mapping strings like 'layer4' → module objects inside the model’s backbone\n",
    "        named_modules = dict(self.model.backbone.named_modules())       # Returns (name, model)\n",
    "\n",
    "        # Checks the requested name exists in the dict — otherwise it raises an error\n",
    "        if target_layer_name not in named_modules:\n",
    "            raise ValueError(f\"Target layer '{target_layer_name}' not found in backbone modules: {list(named_modules.keys())[:10]}...\")\n",
    "\n",
    "        # find target layer module\n",
    "        target_module = dict(self.model.backbone.named_modules())[target_layer_name]\n",
    "\n",
    "        # forward hook to capture activations , output is the activation tensor produced by the model\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()      # .detach() allows to store the raw activation values without further auto-compututaion\n",
    "\n",
    "        # backward hook to capture gradients during nackpropagation\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()   # grad_out contains gradients of outputs wrt to the model's shape(B,C,H,W)\n",
    "\n",
    "        # We save the handles returned by the hooks in self.hook_handles so we can remove hooks later via handle.remove()\n",
    "        self.hook_handles.append(target_module.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(target_module.register_full_backward_hook(backward_hook))\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for h in self.hook_handles:\n",
    "            # Remove hooks so that they don't interfere with fwd/back prop and also to prevent memory leakage\n",
    "            h.remove()\n",
    "\n",
    "    def generate_cam(self, input_tensor: torch.Tensor, class_idx: int = None):\n",
    "        \"\"\"\n",
    "        input_tensor: single sample (1, num_imgs, C, H, W) - chose the first image ;  or (1, C, H, W) - we accept one image only\n",
    "        For simplicity, we produce CAM for the first image in the set (frontal). If input has multiple images,\n",
    "        pass a single image to this function or modify logic accordingly.\n",
    "        \"\"\"\n",
    "        # Reset parameters , gradients , activations\n",
    "        self.model.eval()\n",
    "        self.model.zero_grad()\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # If input has shape (1, n, C, H, W) take first image only\n",
    "        if input_tensor.ndim == 5:\n",
    "            x = input_tensor[:, 0, ...]  # (1, C, H, W)\n",
    "        else:\n",
    "            x = input_tensor\n",
    "\n",
    "        # Get the device(CPU or GPU) where the model params exist\n",
    "        x = x.to(next(self.model.parameters()).device)\n",
    "\n",
    "        # Forward through backbone until logits: we need to call backbone and heads manually\n",
    "        # We'll run through model.backbone and get feature pooled vector\n",
    "        # but to trigger hooks we call the full forward with a crafted forward pass\n",
    "        \n",
    "        # Build a small forward pass:\n",
    "        # We will compute classification logits and pick a class_idx (for binary, positive class)\n",
    "        feats = self.model.backbone(x)          # triggers forward hook on last convolution layer\n",
    "        pooled = feats.view(feats.size(0), -1)  # flattens features to (B,D)\n",
    "        # Run the heads using pooled features\n",
    "        reg_out = self.model.regression_head(pooled)\n",
    "        cls_logits = self.model.classification_head(pooled).squeeze(1)\n",
    "\n",
    "        if class_idx is None:\n",
    "            # for binary, set class_idx based on positive logit\n",
    "            target = cls_logits\n",
    "        else:\n",
    "            # not typical for binary; using direct scalar\n",
    "            target = cls_logits * 0 + cls_logits  # fallback\n",
    "\n",
    "        # Backprop gradients from target scalar\n",
    "        target = target[0]\n",
    "        target.backward(retain_graph=True)\n",
    "\n",
    "        # grads and activations captured\n",
    "        if self.gradients is None or self.activations is None:\n",
    "            raise RuntimeError(\"Gradients or activations not captured — check hooks.\")\n",
    "\n",
    "        # Standard Grad-CAM formula: channel-wise average of gradients used to weight the feature maps, then sum\n",
    "        # grads: (B=1, C, H', W'), activations: same shape\n",
    "        grads = self.gradients[0]       # (C, H', W') , selected the first batch\n",
    "        acts = self.activations[0]      # (C, H', W') , selected the first batch\n",
    "        # Average gradients spatially over (H',W')\n",
    "        weights = grads.mean(dim=(1, 2), keepdim=True)  # (C,1,1)\n",
    "        # Multiply each channel map with its weight\n",
    "        cam = (weights * acts).sum(dim=0)  # (H', W')\n",
    "        # Grad-CAM uses ReLU because negative regions are typically not helpful for \"positive class\" explanation\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam.cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)  # normalize 0..1\n",
    "\n",
    "        # Convert NumPy cam back to a PyTorch tensor and add batch & channel dims\n",
    "        cam_tensor = torch.tensor(cam).unsqueeze(0).unsqueeze(0)  # (1,1,H',W')\n",
    "        # Upsamples the coarse map to the same size as the input image\n",
    "        cam_up = F.interpolate(cam_tensor, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        # Remove extra dimensions added two lines above\n",
    "        cam_up = cam_up.squeeze().numpy()\n",
    "        return cam_up\n",
    "\n",
    "def overlay_cam_on_image(img_tensor, cam_mask, alpha=0.4):\n",
    "    \"\"\"\n",
    "    img_tensor: (C,H,W) un-normalized or normalized (we'll denormalize using ImageNet params)\n",
    "    cam_mask: (H,W) in 0..1\n",
    "    Returns matplotlib figure\n",
    "    \"\"\"\n",
    "    # Denormalize\n",
    "    inv_norm = T.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "\n",
    "    # Move to CPU, denormalize, permute from (C,H,W) to (H,W,C) for plotting, convert to NumPy\n",
    "    img = inv_norm(img_tensor.cpu()).permute(1, 2, 0).numpy()\n",
    "    # ensures RGB stays valid\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(cam_mask, cmap='jet', alpha=alpha)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06707f",
   "metadata": {},
   "source": [
    "# Main Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c62851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(backbone_name: str, pretrained=True, regression_out=3):\n",
    "    if backbone_name == \"resnet50\":\n",
    "        model = ResNet_MultiTaskModel(backbone_name='resnet50', pretrained=pretrained, regression_out=regression_out)\n",
    "    elif backbone_name == \"efficientnet_b3\":\n",
    "        model = EfficientNet_MultiTaskModel(backbone_name='efficientnet_b3', pretrained=pretrained, regression_out=regression_out)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(BACKBONE : str):\n",
    "    cfg = MODEL_CONFIGS[BACKBONE]\n",
    "    IMG_SIZE = cfg[\"img_size\"]\n",
    "    TARGET_LAYER = cfg[\"target_layer\"]\n",
    "\n",
    "    # Ensure regression targets present (pass original labels path if needed)\n",
    "    ensure_targets_in_manifest(MANIFEST, original_labels_path=\"Anthrovision/anthrovision_labels.csv\")\n",
    "\n",
    "    # Load transforms with correct size\n",
    "    train_tf, val_tf = make_transforms(IMG_SIZE)\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(MANIFEST, IMG_ROOT, BATCH_SIZE, NUM_IMAGES_PER_CHILD, train_tf, val_tf)\n",
    "\n",
    "    # Initialize model\n",
    "    model = get_model(BACKBONE, pretrained=True, regression_out=3)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optionally freeze early layers for faster convergence\n",
    "    # Intent: fine-tune only the final block(s) (the ones after TARGET_LAYER) and the heads. \n",
    "    # This can speed up training and reduce overfitting for small datasets\n",
    "    for name, param in model.backbone.named_parameters():\n",
    "        if TARGET_LAYER not in name:  # keep last block trainable\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Pass only trainable parameters to the optimizer, rest are freezed in previous line\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=PATIENCE_LR_SCHED)\n",
    "\n",
    "    # Save the best validation proxy metric\n",
    "    best_val_loss = float('inf')\n",
    "    # Place where best model for this backbone is saved\n",
    "    ckpt_path = f\"best_{BACKBONE}.pth\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, epoch, scheduler=None)\n",
    "        # Runs validation and returns two dictionaries with classification and regression metric\n",
    "        reg_metrics, cls_metrics = evaluate(model, val_loader)\n",
    "\n",
    "        # Compose a val_loss for scheduler\n",
    "        # Use average of normalized regression RMSEs + (1 - roc_auc) as proxy\n",
    "        val_loss_proxy = 0.0\n",
    "        cnt = 0\n",
    "        # Averages the available RMSEs across the three regression targets (ignoring missing targets)\n",
    "        for m in reg_metrics.values():\n",
    "            if m['RMSE'] is not None:\n",
    "                val_loss_proxy += m['RMSE']\n",
    "                cnt += 1\n",
    "        if cnt > 0:\n",
    "            val_loss_proxy = val_loss_proxy / cnt\n",
    "        else:\n",
    "            val_loss_proxy = 0.0\n",
    "\n",
    "        # incorporate classification metric\n",
    "        if cls_metrics['roc_auc'] is not None:\n",
    "            # penalize poor classification AUC\n",
    "            val_loss_proxy += (1.0 - cls_metrics['roc_auc'])\n",
    "\n",
    "        # Instructs the LR scheduler to see the current val_loss_proxy and adjust the LR if plateau detected, stops otherwise\n",
    "        scheduler.step(val_loss_proxy)\n",
    "\n",
    "        # Prints the metrics\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | train_loss: {train_loss:.4f}\")\n",
    "        print(\" Val regression metrics:\")\n",
    "        for k, v in reg_metrics.items():\n",
    "            print(f\"  {k} => MAE: {v['MAE']}, RMSE: {v['RMSE']}, R2: {v['R2']}\")\n",
    "        print(\" Val classification metrics:\", cls_metrics)\n",
    "\n",
    "        # Save best model by val_loss_proxy\n",
    "        if val_loss_proxy < best_val_loss:\n",
    "            best_val_loss = val_loss_proxy\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'config': cfg\n",
    "            }, ckpt_path)\n",
    "            print(\" Saved best model.\")\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)        # loads the checkpoint of the tensor\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])          # restore the trained weights\n",
    "    reg_metrics, cls_metrics = evaluate(model, test_loader)        # get final test metrics\n",
    "    print(\"Final test regression metrics:\")\n",
    "    for k, v in reg_metrics.items():\n",
    "        print(f\"  {k} => MAE: {v['MAE']}, RMSE: {v['RMSE']}, R2: {v['R2']}\")\n",
    "    print(\"Final test classification metrics:\", cls_metrics)\n",
    "\n",
    "    # Example: produce Grad-CAM for 5 test children\n",
    "    print(\"Generating Grad-CAM examples for a few test samples...\")\n",
    "    grad_cam = GradCAM(model)\n",
    "    model.eval()\n",
    "    it = iter(test_loader)\n",
    "    samples = next(it)\n",
    "    images = samples['images']  # (B, n, C, H, W)\n",
    "    # Picks the first batch from the test loader (samples) and the first child in that batch (images[0:1])\n",
    "    first_child_images = images[0:1].to(device)           # (1,n,C,H,W)\n",
    "    cam_mask = grad_cam.generate_cam(first_child_images)  # (IMG_SIZE, IMG_SIZE)\n",
    "    # overlay on first image (denormalize internally)\n",
    "    overlay_cam_on_image(first_child_images[0, 0], cam_mask, alpha=0.5)   # draws the heatmap over the first image from the child\n",
    "    plt.show()\n",
    "    grad_cam.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd54df4",
   "metadata": {},
   "source": [
    "# Train the ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet50 \n",
    "main(BACKBONE = \"resnet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfe9bd",
   "metadata": {},
   "source": [
    "# Train the EfficientNet-B3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EfficientNet-B3 Model\n",
    "main(BACKBONE = \"efficientnet_b3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
